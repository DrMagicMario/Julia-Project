*** YOLO ***
URL: https://www.youtube.com/watch?v=2hAiJe8ITsE&list=WL&index=6
     https://www.youtube.com/watch?v=n9_XyCGr-MI&list=WL&index=2&t=956s

PASCAL VOC Dataset -> use COCO instead


Idea:
  *split image into 7x7 cell and predict a BB and label for each cell
    **label format: [c1,c2,...,cn,Pc,x,y,w,h]
      ***c1 ... cn -> number of classes
      ***Pc -> probability there is object (constrained between 0 and 1)
      ***x,y,w,h -> Bounding Box coordinates
  *only cells responsible for predicting a BB are the ones containing the "center" point of the object being detected
    **output n BB (each will specialize to output differing BB ratios (tall vs wide)
      ***format with 2 BB's: [c1,c2,...,cn,Pc1,x1,y1,w1,h1,Pc2,x2,y2,w2,h2]
      ***note that a cell only has 1 set of class predictions regardless of the number of BB's, thus only one object can be detected per cell. 
    **base coordinates of each cell is the top left corner (0,0) to bottom right corner (1,1)
      ***each output and label will be relative to the cell:
        ****format: [x,y,w,h]
        ****x,y -> object midpoint (constrained between 0 and 1 i.e within the cell) 
        ****w,h -> width,height relative to midpoint (not contrained i.e overlaps cells)
          
Target shape of image: (s,s,25) -> 5B+c = 5(2)+20 = 25
Target prediction of image: (s,s,30) -> 25+5(B)=25+5(1)=30

Model:
  *input: shape 448 x 448 x 3(RGB imgs)

Conv.(7x7x64),stride=2 -> maxpool(2x2),stride=2
Conv.(3x9x192) -> maxpool(2x2),stride=2
Conv.[(1x1x128),
     (3x3x256),
     (1x1x256),
     (3x3,512)] -> maxpool(2x2),stride=2
Conv.[(1x1x256),  -| x4
     (3x3x512),   -|
     (1x1x512),
     (3x3,1024)] -> maxpool(2x2),stride=2
Conv.[(1x1x512),  -| x2
     (3x3x1024),  -|
     (3x3,1024),
     (3x3x1024),stride=2]
Conv.[(3x3x1024),
     (3x3x1024)]
Conn.(7x7x1024) <-linear (4096)-> (7x7x30)


Loss Function:
     
input: image
output: image with a bounding box (BB), label and confidence value

steps:
  1. divide image into grid(matrix) of sxs "cells"
  2. each cell is responsible for predicting BB's 
    - center of BB must be in cell, otherwise the box can overlap with other cells
  3. return BB's above a minimum threshold

how bounding boxes are encoded?
  for each cell the CNN predicts a vector y which contains:
    BB properties: 
      pc: probability bounding box contains an object
      1. (bx,by) = center coordinates, 
      2. (bw,bh) = width,height of BB as percent of cells width and height
    c1...cn: probability that cell belongs to classes 1...n (not BB specific)

what about multiple BBs?
  if we are predicting more than 1 BB per cell -> augment the vector y (concat BB1...BBn)
    -(pc,BB1,...,BBn,c1,...,cn) -> in total y has 5B+c elements, B = # of BB, c = # of classes

total size of output tensor (3D matrix) = S*S*(5B+c) 

Performance? 
  - union over intersection (UoI) -> measures overlap between bounding boxes
  - UoI calculates between predicted BB and ground truth (prelabeled data)

UoI = Area of intersection / Area of Union. 1 = best, 0 = worst

Double counting:
  non-max suppression removes BB with lower confidences that share a label and are above a certain threshold -> max(pred.label)

implementaion:
  - darknet framework trained on ImageNET as feature extractor
  - modified by adding 4 conv. layers and 2 fully connected layers. 
  - 24 total conv layers, 2 fully connected = 26 total layers 

restraint: imput img must be size (448x448)









*** YOLO 9000 (v2)***
URL: https://www.youtube.com/watch?v=GBu2jofRJtk&list=WL&index=6

Yolo_v1:
Anchor boxes: predicts offset from a pre-given box -> YOLO uses soft-max suppression so the offsets tend to be relatively close to anchor boxes. 

Yolo_v2:
To make learning for the network easier, instead of uses pre-defined anchor boxes at different scales:
  - k-means clustering predict offset according to aspect ratio (1:1,2:1,1:3 etc) 
  - images tend to have basic bounding box shapes (better base predictions)
  - 5 BB generates instead of usual 9 Anchor boxes -> easier to train
  - +5% mean average precision (mAP)

Training:
  - Mutli-scale training -> train on images of differing dimensions (multiples of 32) 
  - randomly resized every 10 batches (help with overfitting?)
  - +1.5% mAP

tradeoffs:  
dimensions |^|: FPS |v| , mAP |^| --> @608x608: mAP ~ 80, FPS ~ 40
dimensions |v|: FPS |^| , mAP |v| --> @320x320: mAP ~ 70, FPS ~ 90

9000 classifications:
-combines ImageNET and COCO datasets using miminum spanning tree of WordNet => WordTree.
-WordTree is used to hierachically organize the labels. 
-Ex. COCO label airplane has ImageNet leaves: biplane, jet, airbus, stealth fighter etc.

*note*: images part of ImageNET cannot be judged on BB's since they dont contain them

implementation:
  - Darknet19 -> 19 conv. layers, 5 max pool layers = 24 total layers (feature extractor)









*** YOLO v3 ***
URL: https://www.youtube.com/watch?v=vRqSO6RsptU&list=WL&index=1&t=1s

predicts classes
detects multiple objects 
locations

single NN -> divides img into grid -> assigns cell probabilities -> predicts bounding boxes for detected object 

Terminology:
  - CNN
  - Residual Blocks
  - Skip connections
  - Up-sampling
  - Leaky ReLu
  - IoU
  - Non-maximum suppression
   
Architecture:
 - 53 CNN layers (Darknet-53) stacked with
 - 53 layers
 = 106 layers

  - each layer is followed by batch normalization and Leaky ReLu 
  - no pooling: 53 layers are used with stride 2 to downsample feature maps -> prevents loss of low level features (also helps with detection of smaller objects).

  *pooling only uses some values of the CNN to downsample but using the CNN itself maintains all the information

Input:
  - batch of images with shape (n=number_of_images,416=width,416=height,3=channels=RGB)
  - tuple (width,height) = network size
  - images do not need to be resized

  restraints: 
    - (width, height % 32 == 0) == true.
     
***Feature extractor***
Variant of Darknet53 (53 conv layers + residual/short-cut connections) is used.
  - 106 full conv. NN (FCNN) -> detection kernels + downsampling  @ layers 82,94,106

***Downsampling:***
input: 32 @ layer 82, 16 @ layer 94 , 8 @ layer 108 ("network strides")

output size: network_size./stride 
  - input->416x416, stride=32, output=416/32=13, output->13x13 **** detects large objects
  - input->416x416, stride=16, output=416/16=26, output->26x26 **** detects med. objects
  - input->416x416, stride=8, output=416/8=52, output->52x52 **** detects small objects

***Detection Kernels***
  - 1x1 -> output size= network_size./stride
      shape: b*(5+c), b=3,
          - b is the number of bounding boxes that each cell of produced feature map can predict. 
          - Each box has 5+c attributes. 
        
      COCO = 80 classes - > 5+c = 85; therfore for COCO: 3*85 = 255 attributes

  - produce feature maps that encode the attributes
      shape: (13,13,255), (26,26,255), (52,52,255) for COCO
      A bounding box (BB) is predicted if the cell in the center of the box belongs to the receptive field

***Training***
*COCO*:: default BB ("anchors" or "priors") calculated using k-means clustering.
  @ layer 82 ->  BB1:(116x90), BB2:(156x198), BB3:(373x326) -> number of predictions: 507
  @ layer 94 ->  BB1:(30x61), BB2:(62x45), BB3:(59x119) -> number of predictions: 2028
  @ layer 106 ->  BB1:(10x13), BB2:(16x30), BB3:(33x23) -> number of predictions: 8112

9 total BB, 10647 total predictions for input 416x416 using COCO

how do we assign best prediction? extract probabilities
  - compute element wise product of objectness scores and confidences.
    * p0 = objectness score: 1 = center cell, 0 = corner cell (probability that cell contains an object)
      ** Pobject * IoU -> sigmoid(t0) -> p0, 
        Pobject = probability BB contains object
        Intersection over union (IoU) = (BB1 n BB2)/(BB1 u BB2), BB2 = predicted, BB1 = ground truth
    * p1,p2,...,pc = confidences for each class of object
  - find max(probabilities)
  - return prediction

Forward Pass:
yolov3 calculates offsets of width and height to these predefined anchors (log-space transform). Helps eliminate unstable gradients

  width = pw * e^(th) 
  height = ph * e^(tw)

yolov3 calculates the center coordinate of a bounding box using the sigmoid function 
  x = sigmoid(tx) + cx
  y = sigmoid(ty) + cy

cx, cy = top left coordinate of anchor box
tx,ty,tw,th = output of NN
pw,ph = anchor box width and height

cx, cy, pw and ph have to be normalized:
  cx = cx/width
  cy = cy/height
  pw = pw/width
  py = py/height

**Summary***
  - Applies CNN to image
  - Downsamples image at 3 scales
  - 1x1 detection kernels are applied to grid cells at 3 scales
  - 1 cell is responsible for detecting 1 object - 9 BB used (anchors - 3 for each scale)
  - 10647 BB predicted (13*13+26*26+52*52*3)









*** YOLO v4 ***
URL: https://www.youtube.com/watch?v=bDK9NRF20To&list=WL&index=5&t=15s

Basic Architecture of Common object detector:

  1. Inputs: Image, Patches, Image Pyramid
  2. Backbones: VGG16, ResNet-50, SpineNet, EfficientNet, CSPResNeXt50, CSPDarknet53
  3. Neck (detector): 
      -> additional blocks: SPP, ASPP, RFB, SAM
      -> Path-aggregator blocks: PAN, FPN, NAS-FPN, Fully Connectd FPN, BiFPN, ASFF, SFAM
  4. Head:
      -> Dense Prediction (one-stage): RPN, SSD, YOLO, RetinaNet(anchor based), CornerNet, CenterNEt, MatrixNet, FCOS(anchor free)
      -> Sparse prediction (two-stage): Faster R-CNN, R-FCN, Mask R-CNN (anchor based), RepPoints (anchor free)

########## training strategies for improved learning with more training cost ##########

  * Normalization of activations by mean and variance
    ** Batch normalization (BN), Cross-GPU (CGBN or SyncBN), Filter Response (FPN), Cross-iteration (CBN)

  * Regularization (feature map)
    ** Object occlusion
      *** Ease, CutOut, Zero mask, Grid mask, DropOut, DropConnect, DropBlock (all very similar)

  * Data Augmentation
    ** object occlusion
    ** GAN
    ** Multiple Images
    ** Single Image
      *** Photometric distortions
        **** Brightness, Contrast, Hue, Saturation, Noise
      *** Geometric distortions
        **** Scaling, Cropping, Flipping, Rotating

  * Data Imbalance
    ** Two-stage detector
      *** Hard negative example mining, Online hard example mining
    ** Focal loss

  * Degree of associative between categories
    ** Softlabel (label smoothing), Label refinement network (knowledge distillation)

  * Objective function of BB regression
    ** IoU loss, GIoU (shape + orientation), DIoU loss (distance + shape + orientation)


########## Plugin/Post-processing to improve accuracy with low inference cost  ##########

  * Enlarging receptive Field
    ** SPP, ASPP, RFB

  * Attention module
   ** Channel-wise
    *** Squeeze-and-Excitation (SE)
   ** Spatial-wise 
    *** Special Attention Module (SAM)

  * Feature integration
    ** Skip-connection, Hyper-column, FPN, SFAM, ASFF, BiFPN

  * Post-processing
    ** Activation function
      *** ReLU, LReLU, PreLU, ReLU6, SELU, Swish, Hard-Swish, Mish 
    ** Anchored
      *** NMS, Soft-NMS, DIoU NMS, CIoU NMS

########## Architecture of YoloV4  ##########

Backbone: CSPDarknet53

  Training: 
    * Data Augmentation
      ** Mosaic, CutMix
    * Regularization (feature map)
      ** DropBlock
    * Degree of associative between categories
      ** Class label smoothing
  Accuracy:
    * Post-processing
      ** Activation function
        *** Mish activation
    * Cross-stage partial connections (CSP)
      - Splits layer in 2, 1 -> CNN, 2 -> Dense block. The results are aggregated before passing it to the next layer. 
    * Multi-input weighted residual connections (MiWRC)

Neck (Detector): SPP + PANet
  Training: 
    * Data Augmentation
      ** Mosaic, Self-Adversarial
    * Objective function of BB regression
      ** CIoU-loss, 
    * Eliminate grid sensitivity
    * Multiple achors for single ground truth
    * Cosine annealing scheduler
    * Optimal hyper-parameters
    * Random training shapes
  Accuracy:
    * Post-processing
      ** Activation function
        *** Mish activation
      ** Anchored
        *** NMS, Soft-NMS, DIoU NMS, CIoU NMS
    * Enlarging receptive Field
      ** Spatial Pyramid Pool (SPP)
    * Attention module
     ** Spatial-wise 
      *** Special Attention Module (SAM)
      *** Path Aggregation Netwrok (PAN)

Head: YOLOv3
